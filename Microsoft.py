#!/usr/bin/env python
# coding: utf-8


# noinspection PyInterpreter
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt 
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
import lightgbm as lgb
from sklearn.metrics import classification_report,confusion_matrix,accuracy_score


train_data = pd.DataFrame()
for chunk in pd.read_csv('train.csv',chunksize=100000):
    train_data = pd.concat([train_data,pd.DataFrame(chunk)])
    print(train_data)



null_perc = pd.DataFrame(train_data.isnull().sum()/train_data.shape[0]*100)


train_data.drop(list(null_perc[null_perc>60].dropna().index),axis=1,inplace=True)

col_objects = list()
for i in list(train_data.columns):
    if train_data[i].dtype == np.object:
        col_objects.append(i)


def unique(x):
    unique = [i for i in list(train_data[x].unique()) if i!=np.nan]
    col_unique = {unique[j]:j for j in range(len(unique))}
    return col_unique


for i in col_objects[1:]:
    print(i)
    train_data[i] = train_data[i].replace(unique(i))

scale = StandardScaler()
train_data[[i for i in list(train_data.columns) if i not in ['MachineIdentifier','HasDetections']]]\
    =scale.fit_transform(train_data[[i for i in list(train_data.columns) if i not in ['MachineIdentifier','HasDetections']]])

for i in [i for i in list(train_data.columns) if i not in ['MachineIdentifier','HasDetections']]:
    train_data[i] = train_data[i].fillna(train_data[i].median())


train_without_machine = train_data.drop(['MachineIdentifier','HasDetections'],axis=1)
train_machine = pd.DataFrame(train_data['MachineIdentifier'],columns=['MachineIdentifier'])
train_label = pd.DataFrame(train_data['HasDetections'],columns=['HasDetections'])


pca = PCA(n_components=25)
train_without_machine = pd.DataFrame(pca.fit_transform(train_without_machine),
                                    columns=["Feature_"+str(i) for i in range(1,26)])

final_train = train_machine.join(train_without_machine).join(train_label)


final_train.to_csv('final_train.csv',index=False)


X = final_train.drop(['MachineIdentifier','HasDetections'],axis=1)
y = final_train['HasDetections']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=101)


lgb_train = lgb.Dataset(X_train,label=y_train)


params = {}
params['learning_rate'] = 0.0001
params['boosting_type'] = 'gbdt'
params['objective'] = 'binary'
params['metric'] = 'binary_logloss'
params['sub_feature'] = 0.5
params['num_leaves'] = 100
params['min_data'] = 40
params['max_depth'] = 7
params['max_bin'] = 30


clf = lgb.train(params,lgb_train,300)

lgb.plot_importance(clf)
plt.show()

pred = clf.predict(X_test)

for i in range(len(pred)):
    if pred[i]>=0.5:pred[i]=1
    else:pred[i]=0


print(classification_report(y_test,pred))


accuracy_score(y_test,pred)



